    payload = {'api_key': '37277f0e22d9948e62e5708337e51782', 'url': url}
    r = requests.get('http://api.scraperapi.com', params=payload)
    soup = BeautifulSoup(r.text, 'html.parser')
    product_titles = soup.find_all('p', {'data-test-id': 'productTitle'})
    product_prices = soup.find_all('div', {'data-test-id': 'product-primary-price'})
    category_title_html = soup.find('h1', {'data-test-id': 'plp-title'})
    product_links = soup.find_all('a', {'data-test-id': 'product-panel-main-section'})
    if category_title_html is not None:
        category_title = category_title_html.text
    else:
        category_title = "N/A"
    product_list = []
    price_list = []
    link_list = []
    for product_title in product_titles:
        title_element = product_title.text
        if title_element:
            product_list.append(title_element)

    for product_price in product_prices:
        text = ""
        text += product_price.get_text(strip=True)
        if text.strip() == "":
            text = "N/A"
        else:
            price_list.append(text)

    for product_link in product_links:
        link_list.append('https://www.diy.com' + product_link['href'])

    for product, price, link in zip(product_list, price_list, link_list):
        products.append({'product': product, 'price': price, 'category': category_title, 'link': link})

# r = requests.post(url='https://async.scraperapi.com/batchjobs',
#                   json={'apiKey': '37277f0e22d9948e62e5708337e51782',
#                         'urls': urls})

print(products)
df = pd.DataFrame(products)
df.to_csv('products.csv', index=False)